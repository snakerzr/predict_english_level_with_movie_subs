{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad640ee-a543-4b1f-8ad4-53dd945e9b0e",
   "metadata": {},
   "source": [
    "–¢–∞–º –Ω–∞–¥–æ –≤–æ –ø–µ—Ä–≤—ã—Ö \n",
    "* –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ —Ç–µ–∫—Å—Ç, \n",
    "* –∑–∞—Ç–µ–º —Ä–∞–∑–º–µ—Å—Ç–∏—Ç—å —ç—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –ø–∞–ø–∫–∞–º —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏, —á—Ç–æ–±—ã —Å–∫–ª–µ—Ä–Ω –º–æ–≥ –∏—Ö –≤–∑—è—Ç—å –∫–∞–∫ X,y, \n",
    "* –∑–∞—Ç–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å X (—Ç.–µ. —Ç–µ–∫—Å—Ç —Å–∞–º), \n",
    "* –ø–æ—Ç–æ–º –µ—â–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –≤—ã—Ç–∞—â–∏—Ç—å –∫–∞–∫—É—é-—Ç–æ –º–µ—Ç–∞ –∏–Ω—Ñ—É, —Ç–∏–ø–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—è–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤ –Ω–∞ —Å—É–±—Ç–∏—Ç—Ä–∞—Ö –∏ —Ç.–¥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ba040-a435-41c4-b837-759a5d58b1dc",
   "metadata": {},
   "source": [
    "1. –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ñ–∏–ª—å–º–∞, —É—Ä–æ–≤–Ω–µ–º –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ñ–∞–π–ª–∞ —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏\n",
    "    * –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å—Ç–æ–ª–±—Ü—ã\n",
    "    * –ü—Ä–∏–º–µ–Ω—è–µ–º `.strip()` –∫ –Ω–∞–∑–≤–∞–Ω–∏—é —Ñ–∏–ª—å–º–æ–≤ (—Ç–∞–º –±—ã–ª–∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫)\n",
    "    * –ó–∞–º–µ–Ω—è–µ–º `/` –Ω–∞ `_` –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —É—Ä–æ–≤–Ω–µ–π\n",
    "    * –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏ —Å –ø–æ–º–æ—â—å—é `os.listdir()`\n",
    "    * –° –ø–æ–º–æ—â—å—é `difflib.get_close_matches()` —Å–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ —Ñ–∞–π–ª–æ–≤ —Å—É–±—Ç–∏—Ç—Ä–æ–≤\n",
    "2. –°–æ–∑–¥–∞–¥–∏–º –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Å—É–±—Ç–∏—Ç—Ä–∞ plain text (.txt) —Å –ø–æ–º–æ—â—å—é `pysubs2` –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –ø–∞–ø–∫–µ\n",
    "3. –†–∞—Å–ø—Ä–µ–¥–µ–ª–∏–º txt –ø–æ –ø–æ–¥–ø–∞–ø–∫–∞–º —Å —É—Ä–æ–≤–Ω–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã `sklearn.datasets.load_files`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb6cb1-38b2-40a4-ade2-c59e0fe7a1b6",
   "metadata": {},
   "source": [
    "## –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ñ–∏–ª—å–º–∞, —É—Ä–æ–≤–Ω–µ–º –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ñ–∞–π–ª–∞ —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3197a1-b3bb-44d5-bc00-f8ddee092e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "import difflib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pysubs2\n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9888a108-1764-4e48-9fc9-5e246470bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_preproc(path):\n",
    "    labels = pd.read_csv(path)\n",
    "    # labels.columns\n",
    "    labels = labels.drop(columns=['Kinopoisk ','Subtitles'])\n",
    "    labels = labels.sort_values('Movie').reset_index(drop=True)\n",
    "\n",
    "    labels['Movie'] = labels['Movie'].str.strip()\n",
    "\n",
    "    # with pd.option_context(\"display.max_rows\", 300):\n",
    "    #     display(labels['Movie'])\n",
    "    # labels['Movie'].unique()\n",
    "    # labels['Level'].value_counts()\n",
    "\n",
    "    labels.loc[labels['Level'] == 'A2/A2+','Level'] = 'A2_A2+'\n",
    "    # labels.loc[labels['Level'] == 'B1, B2','Level'] = 'A2_A2+'\n",
    "    labels.loc[labels['Level'] == 'A2/A2+, B1','Level'] = 'A2_A2+, B1'\n",
    "    return labels\n",
    "\n",
    "labels = labels_preproc('data/labels.csv')\n",
    "# labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "533d975b-e163-4377-8687-a0b81690fa29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_subs_file_names(df,path_to_raw_subs):\n",
    "    # –°–ø–∏—Å–æ–∫ —É–∂–µ –∏–º–µ—é—â–∏—Ö—Å—è —Å—É–±—Ç–∏—Ç—Ä–æ–≤\n",
    "    subs_file_list = os.listdir(path_to_raw_subs)\n",
    "    # subs_file_list = [x.lower() for x in subs_file_list]\n",
    "    # subs_file_list[:5]\n",
    "    # labels.head(1)\n",
    "\n",
    "    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—É–±—Ç–∏—Ç—Ä–∞ –ø–æ –Ω–∞–∏–±–æ–ª—å—à–µ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é\n",
    "    for movie in df['Movie']:\n",
    "        df.loc[df['Movie'] == movie,'sub_file_name'] = difflib.get_close_matches(movie, subs_file_list,cutoff=0)[0]\n",
    "        \n",
    "    return df\n",
    "        \n",
    "labels = add_subs_file_names(labels,'data/Subtitles_raw/')\n",
    "labels = labels.drop(index=[39,61]) # –£–¥–∞–ª—è–µ–º —Å–µ—Ä–∏–∞–ª—ã\n",
    "labels.loc[labels['Movie'] == 'Harry Potter (1)', 'sub_file_name'] = 'Harry_Potter_and_the_philosophers_stone(2001).srt'\n",
    "labels = labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "931986bb-3166-44b5-acdb-d800bc67d72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "      <th>sub_file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Cloverfield Lane</td>\n",
       "      <td>B1</td>\n",
       "      <td>10_Cloverfield_lane(2016).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 things I hate about you</td>\n",
       "      <td>B1</td>\n",
       "      <td>10_things_I_hate_about_you(1999).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A knight‚Äôs tale</td>\n",
       "      <td>B2</td>\n",
       "      <td>A_knights_tale(2001).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A star is born</td>\n",
       "      <td>B2</td>\n",
       "      <td>A_star_is_born(2018).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Aladdin(1992).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All dogs go to heaven</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>All_dogs_go_to_heaven(1989).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An  American tail</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>An_American_tail(1986).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Babe</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Babe(1995).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Back to the future</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Back_to_the_future(1985).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Batman begins</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Batman_begins(2005).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Beauty and the beast (film)</td>\n",
       "      <td>B2</td>\n",
       "      <td>Beauty_and_the_beast(2017).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Before I go to sleep</td>\n",
       "      <td>B2</td>\n",
       "      <td>Before_I_go_to_sleep(2014).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Before sunrise</td>\n",
       "      <td>B1, B2</td>\n",
       "      <td>Before_sunrise(1995).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Before sunset</td>\n",
       "      <td>B1, B2</td>\n",
       "      <td>Before_sunset(2004).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Braveheart</td>\n",
       "      <td>B2</td>\n",
       "      <td>Braveheart(1995).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bridget Jones diary</td>\n",
       "      <td>B1, B2</td>\n",
       "      <td>Bridget_Jones_diary(2001).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cast away</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Cast_away(2000).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Catch me if you can</td>\n",
       "      <td>B1, B2</td>\n",
       "      <td>Catch_me_if_you_can(2002).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Clueless</td>\n",
       "      <td>B1</td>\n",
       "      <td>Clueless(1995).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Deadpool</td>\n",
       "      <td>B2</td>\n",
       "      <td>Deadpool(2016).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Die Hard</td>\n",
       "      <td>B1</td>\n",
       "      <td>Die_hard(1988).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Dredd</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Dredd(2012).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Dune</td>\n",
       "      <td>B2</td>\n",
       "      <td>Dune(2021).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Eurovision Song Contest: The Story of Fire Saga</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Eurovision_song_contest_(2020).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fight club</td>\n",
       "      <td>B2</td>\n",
       "      <td>Fight_club(1999).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Finding Nemo</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Finding_Nemo(2003).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>A2_A2+, B1</td>\n",
       "      <td>Forrest_Gump(1994).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Good Will Hunting</td>\n",
       "      <td>B2</td>\n",
       "      <td>Good_Will_Hunting(1997).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Groundhog day</td>\n",
       "      <td>B1</td>\n",
       "      <td>Groundhog_day(1993).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Harry Potter (1)</td>\n",
       "      <td>B1</td>\n",
       "      <td>Harry_Potter_and_the_philosophers_stone(2001).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Her</td>\n",
       "      <td>A2_A2+, B1</td>\n",
       "      <td>Her(2013).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Home alone</td>\n",
       "      <td>B1</td>\n",
       "      <td>Home_alone(1990).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Hook</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Hook(1991).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>House of Gucci</td>\n",
       "      <td>B2</td>\n",
       "      <td>House_of_Gucci(2021).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Inside out</td>\n",
       "      <td>B1</td>\n",
       "      <td>Inside_out(2015).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>It‚Äôs a wonderful life</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>It_s_a_wonderful_life(1946).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Knives out</td>\n",
       "      <td>B2</td>\n",
       "      <td>Knives_out(2019).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Kubo and the two strings</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Kubo_and_the_two_strings(2016).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Liar, liar</td>\n",
       "      <td>B1</td>\n",
       "      <td>Liar_liar(1997).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Lion</td>\n",
       "      <td>B1, B2</td>\n",
       "      <td>Lion(2016).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Logan</td>\n",
       "      <td>B1</td>\n",
       "      <td>Logan(2017).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Love actually</td>\n",
       "      <td>B1</td>\n",
       "      <td>Love_actually(2003).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Mamma Mia</td>\n",
       "      <td>B1</td>\n",
       "      <td>Mamma_Mia(2008).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Mary Poppins Returns</td>\n",
       "      <td>B1</td>\n",
       "      <td>Mary_Poppins_returns(2018).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Matilda</td>\n",
       "      <td>B1</td>\n",
       "      <td>Matilda(1996).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Meet the parents</td>\n",
       "      <td>B1</td>\n",
       "      <td>Meet_the_parents(2000).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Moulin Rouge üéôÔ∏è</td>\n",
       "      <td>A2_A2+, B1</td>\n",
       "      <td>Moulin_Rouge(2001).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Mrs. Doubtfire</td>\n",
       "      <td>B1</td>\n",
       "      <td>Mrs_Doubtfire(1993).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>My big fat Greek wedding</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>My_big_fat_Greek_wedding(2002).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Notting Hill</td>\n",
       "      <td>B1</td>\n",
       "      <td>Notting_Hill(1999).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Pirates of the Caribbean (1)</td>\n",
       "      <td>B1</td>\n",
       "      <td>Pirates_of_the_Caribbean(2003).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Pleasantville</td>\n",
       "      <td>B1</td>\n",
       "      <td>Pleasantville(1998).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Powder</td>\n",
       "      <td>B1</td>\n",
       "      <td>Powder(1995).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>B2</td>\n",
       "      <td>Pulp_fiction(1994).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Ready or not</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Ready_or_not(2019).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Shrek</td>\n",
       "      <td>B1</td>\n",
       "      <td>Shrek(2001).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Sleepless in Seattle</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Sleepless_in_Seattle(1993).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Soul</td>\n",
       "      <td>B1</td>\n",
       "      <td>Soul(2020).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>The Greatest Showman üéôÔ∏è</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>The_greatest_showman(2017).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>B1, B2</td>\n",
       "      <td>The_Shawshank_redemption(1994).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>The blind side</td>\n",
       "      <td>B1</td>\n",
       "      <td>The_blind_side(2009).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>The break-up</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>The_break-up(2006).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>The cabin in the woods</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>The_cabin_in_the_woods(2012).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>The fault in our stars üò≠</td>\n",
       "      <td>A2_A2+, B1</td>\n",
       "      <td>The_fault_in_our_stars(2014).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>The graduate</td>\n",
       "      <td>B1, B2</td>\n",
       "      <td>The_graduate(1967).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>The hangover</td>\n",
       "      <td>B2</td>\n",
       "      <td>The_hangover(2009).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>The holiday</td>\n",
       "      <td>B1</td>\n",
       "      <td>The_holiday(2006).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>The invisible man (2020)</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>The_invisible_man(2020).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>The jungle book</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>The_jungle_book(2016).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The king‚Äôs speech</td>\n",
       "      <td>B2</td>\n",
       "      <td>The_kings_speech(2010).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>The lion king</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>The_lion_king(1994).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>The lord of the rings</td>\n",
       "      <td>B1, B2</td>\n",
       "      <td>The_lord_of_the_rings(2001).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>The man called Flinstone</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>The_man_called_Flintstone(1966).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>The secret life of Walter Mitty</td>\n",
       "      <td>B1</td>\n",
       "      <td>The_secret_life_of_Walter_Mitty(2013).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>The social network</td>\n",
       "      <td>B2</td>\n",
       "      <td>The_social_network(2010).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>The terminal</td>\n",
       "      <td>A2_A2+, B1</td>\n",
       "      <td>The_terminal(2004).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>The terminator</td>\n",
       "      <td>B1</td>\n",
       "      <td>The_terminator(1984).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>The theory of everything</td>\n",
       "      <td>B2</td>\n",
       "      <td>The_theory_of_everything(2014).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>The usual suspects</td>\n",
       "      <td>B2</td>\n",
       "      <td>The_usual_suspects(1995).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Titanic</td>\n",
       "      <td>B2</td>\n",
       "      <td>Titanic(1997).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Toy story</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Toy_story(1995).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Twilight(2008).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Up</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Up(2009).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Venom</td>\n",
       "      <td>B2</td>\n",
       "      <td>Venom(2018).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Warm bodies</td>\n",
       "      <td>B1</td>\n",
       "      <td>Warm_bodies(2013).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>We‚Äôre the Millers</td>\n",
       "      <td>B1</td>\n",
       "      <td>We_are_the_Millers(2013).srt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Movie       Level  \\\n",
       "0                               10 Cloverfield Lane          B1   \n",
       "1                        10 things I hate about you          B1   \n",
       "2                                   A knight‚Äôs tale          B2   \n",
       "3                                    A star is born          B2   \n",
       "4                                           Aladdin      A2_A2+   \n",
       "5                             All dogs go to heaven      A2_A2+   \n",
       "6                                 An  American tail      A2_A2+   \n",
       "7                                              Babe      A2_A2+   \n",
       "8                                Back to the future      A2_A2+   \n",
       "9                                     Batman begins      A2_A2+   \n",
       "10                      Beauty and the beast (film)          B2   \n",
       "11                             Before I go to sleep          B2   \n",
       "12                                   Before sunrise      B1, B2   \n",
       "13                                    Before sunset      B1, B2   \n",
       "14                                       Braveheart          B2   \n",
       "15                              Bridget Jones diary      B1, B2   \n",
       "16                                        Cast away      A2_A2+   \n",
       "17                              Catch me if you can      B1, B2   \n",
       "18                                         Clueless          B1   \n",
       "19                                         Deadpool          B2   \n",
       "20                                         Die Hard          B1   \n",
       "21                                            Dredd      A2_A2+   \n",
       "22                                             Dune          B2   \n",
       "23  Eurovision Song Contest: The Story of Fire Saga      A2_A2+   \n",
       "24                                       Fight club          B2   \n",
       "25                                     Finding Nemo      A2_A2+   \n",
       "26                                     Forrest Gump  A2_A2+, B1   \n",
       "27                                Good Will Hunting          B2   \n",
       "28                                    Groundhog day          B1   \n",
       "29                                 Harry Potter (1)          B1   \n",
       "30                                              Her  A2_A2+, B1   \n",
       "31                                       Home alone          B1   \n",
       "32                                             Hook      A2_A2+   \n",
       "33                                   House of Gucci          B2   \n",
       "34                                       Inside out          B1   \n",
       "35                            It‚Äôs a wonderful life      A2_A2+   \n",
       "36                                       Knives out          B2   \n",
       "37                         Kubo and the two strings      A2_A2+   \n",
       "38                                       Liar, liar          B1   \n",
       "39                                             Lion      B1, B2   \n",
       "40                                            Logan          B1   \n",
       "41                                    Love actually          B1   \n",
       "42                                        Mamma Mia          B1   \n",
       "43                             Mary Poppins Returns          B1   \n",
       "44                                          Matilda          B1   \n",
       "45                                 Meet the parents          B1   \n",
       "46                                  Moulin Rouge üéôÔ∏è  A2_A2+, B1   \n",
       "47                                   Mrs. Doubtfire          B1   \n",
       "48                         My big fat Greek wedding      A2_A2+   \n",
       "49                                     Notting Hill          B1   \n",
       "50                     Pirates of the Caribbean (1)          B1   \n",
       "51                                    Pleasantville          B1   \n",
       "52                                           Powder          B1   \n",
       "53                                     Pulp Fiction          B2   \n",
       "54                                     Ready or not      A2_A2+   \n",
       "55                                            Shrek          B1   \n",
       "56                             Sleepless in Seattle      A2_A2+   \n",
       "57                                             Soul          B1   \n",
       "58                          The Greatest Showman üéôÔ∏è      A2_A2+   \n",
       "59                         The Shawshank Redemption      B1, B2   \n",
       "60                                   The blind side          B1   \n",
       "61                                     The break-up      A2_A2+   \n",
       "62                           The cabin in the woods      A2_A2+   \n",
       "63                         The fault in our stars üò≠  A2_A2+, B1   \n",
       "64                                     The graduate      B1, B2   \n",
       "65                                     The hangover          B2   \n",
       "66                                      The holiday          B1   \n",
       "67                         The invisible man (2020)      A2_A2+   \n",
       "68                                  The jungle book      A2_A2+   \n",
       "69                                The king‚Äôs speech          B2   \n",
       "70                                    The lion king      A2_A2+   \n",
       "71                            The lord of the rings      B1, B2   \n",
       "72                         The man called Flinstone      A2_A2+   \n",
       "73                  The secret life of Walter Mitty          B1   \n",
       "74                               The social network          B2   \n",
       "75                                     The terminal  A2_A2+, B1   \n",
       "76                                   The terminator          B1   \n",
       "77                         The theory of everything          B2   \n",
       "78                               The usual suspects          B2   \n",
       "79                                          Titanic          B2   \n",
       "80                                        Toy story      A2_A2+   \n",
       "81                                         Twilight      A2_A2+   \n",
       "82                                               Up      A2_A2+   \n",
       "83                                            Venom          B2   \n",
       "84                                      Warm bodies          B1   \n",
       "85                                We‚Äôre the Millers          B1   \n",
       "\n",
       "                                        sub_file_name  \n",
       "0                       10_Cloverfield_lane(2016).srt  \n",
       "1                10_things_I_hate_about_you(1999).srt  \n",
       "2                            A_knights_tale(2001).srt  \n",
       "3                            A_star_is_born(2018).srt  \n",
       "4                                   Aladdin(1992).srt  \n",
       "5                     All_dogs_go_to_heaven(1989).srt  \n",
       "6                          An_American_tail(1986).srt  \n",
       "7                                      Babe(1995).srt  \n",
       "8                        Back_to_the_future(1985).srt  \n",
       "9                             Batman_begins(2005).srt  \n",
       "10                     Beauty_and_the_beast(2017).srt  \n",
       "11                     Before_I_go_to_sleep(2014).srt  \n",
       "12                           Before_sunrise(1995).srt  \n",
       "13                            Before_sunset(2004).srt  \n",
       "14                               Braveheart(1995).srt  \n",
       "15                      Bridget_Jones_diary(2001).srt  \n",
       "16                                Cast_away(2000).srt  \n",
       "17                      Catch_me_if_you_can(2002).srt  \n",
       "18                                 Clueless(1995).srt  \n",
       "19                                 Deadpool(2016).srt  \n",
       "20                                 Die_hard(1988).srt  \n",
       "21                                    Dredd(2012).srt  \n",
       "22                                     Dune(2021).srt  \n",
       "23                 Eurovision_song_contest_(2020).srt  \n",
       "24                               Fight_club(1999).srt  \n",
       "25                             Finding_Nemo(2003).srt  \n",
       "26                             Forrest_Gump(1994).srt  \n",
       "27                        Good_Will_Hunting(1997).srt  \n",
       "28                            Groundhog_day(1993).srt  \n",
       "29  Harry_Potter_and_the_philosophers_stone(2001).srt  \n",
       "30                                      Her(2013).srt  \n",
       "31                               Home_alone(1990).srt  \n",
       "32                                     Hook(1991).srt  \n",
       "33                           House_of_Gucci(2021).srt  \n",
       "34                               Inside_out(2015).srt  \n",
       "35                    It_s_a_wonderful_life(1946).srt  \n",
       "36                               Knives_out(2019).srt  \n",
       "37                 Kubo_and_the_two_strings(2016).srt  \n",
       "38                                Liar_liar(1997).srt  \n",
       "39                                     Lion(2016).srt  \n",
       "40                                    Logan(2017).srt  \n",
       "41                            Love_actually(2003).srt  \n",
       "42                                Mamma_Mia(2008).srt  \n",
       "43                     Mary_Poppins_returns(2018).srt  \n",
       "44                                  Matilda(1996).srt  \n",
       "45                         Meet_the_parents(2000).srt  \n",
       "46                             Moulin_Rouge(2001).srt  \n",
       "47                            Mrs_Doubtfire(1993).srt  \n",
       "48                 My_big_fat_Greek_wedding(2002).srt  \n",
       "49                             Notting_Hill(1999).srt  \n",
       "50                 Pirates_of_the_Caribbean(2003).srt  \n",
       "51                            Pleasantville(1998).srt  \n",
       "52                                   Powder(1995).srt  \n",
       "53                             Pulp_fiction(1994).srt  \n",
       "54                             Ready_or_not(2019).srt  \n",
       "55                                    Shrek(2001).srt  \n",
       "56                     Sleepless_in_Seattle(1993).srt  \n",
       "57                                     Soul(2020).srt  \n",
       "58                     The_greatest_showman(2017).srt  \n",
       "59                 The_Shawshank_redemption(1994).srt  \n",
       "60                           The_blind_side(2009).srt  \n",
       "61                             The_break-up(2006).srt  \n",
       "62                   The_cabin_in_the_woods(2012).srt  \n",
       "63                   The_fault_in_our_stars(2014).srt  \n",
       "64                             The_graduate(1967).srt  \n",
       "65                             The_hangover(2009).srt  \n",
       "66                              The_holiday(2006).srt  \n",
       "67                        The_invisible_man(2020).srt  \n",
       "68                          The_jungle_book(2016).srt  \n",
       "69                         The_kings_speech(2010).srt  \n",
       "70                            The_lion_king(1994).srt  \n",
       "71                    The_lord_of_the_rings(2001).srt  \n",
       "72                The_man_called_Flintstone(1966).srt  \n",
       "73          The_secret_life_of_Walter_Mitty(2013).srt  \n",
       "74                       The_social_network(2010).srt  \n",
       "75                             The_terminal(2004).srt  \n",
       "76                           The_terminator(1984).srt  \n",
       "77                 The_theory_of_everything(2014).srt  \n",
       "78                       The_usual_suspects(1995).srt  \n",
       "79                                  Titanic(1997).srt  \n",
       "80                                Toy_story(1995).srt  \n",
       "81                                 Twilight(2008).srt  \n",
       "82                                       Up(2009).srt  \n",
       "83                                    Venom(2018).srt  \n",
       "84                              Warm_bodies(2013).srt  \n",
       "85                       We_are_the_Millers(2013).srt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with pd.option_context(\"display.max_rows\", 300):\n",
    "    display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb17791-77db-4c07-8a4b-4270bcedf994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "055f69fc-2c25-4e65-ba5a-3e88d4306bee",
   "metadata": {},
   "source": [
    "## –°–æ–∑–¥–∞–¥–∏–º –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Å—É–±—Ç–∏—Ç—Ä–∞ plain text (.txt) —Å –ø–æ–º–æ—â—å—é `pysubs2` –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –ø–∞–ø–∫–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab63071-6a54-4163-a7a5-9f35f3695fca",
   "metadata": {},
   "source": [
    "–£–¥–∞–ª–∏—Ç—å HTML —Ç–µ–≥–∏ –ø—Ä–æ—Å—Ç–æ\n",
    "https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python#:~:text=If%20you%20need%20to%20strip%20HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c00fc03-a0d5-478d-8832-03833747147a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_plain_text_str(sub_file,delete_html_tags=True):\n",
    "    plain_text = ''\n",
    "    for line in sub_file:  \n",
    "        plain_text += line.plaintext + ' '\n",
    "        # plain_text += line.text + ' '\n",
    "        \n",
    "    if delete_html_tags:\n",
    "        plain_text = re.sub('<[^<]+?>', ' ', plain_text)\n",
    "        \n",
    "    return plain_text\n",
    "\n",
    "\n",
    "# subs = pysubs2.load('data/Subtitles_raw/10_Cloverfield_lane(2016).srt',\n",
    "#                     # keep_html_tags=False,\n",
    "#                     keep_unknown_html_tags=True\n",
    "#                    )\n",
    "\n",
    "# test = create_plain_text_str(subs)\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e908ec3a-572a-4aa9-b2f5-bb05a4f57085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Subtitles_plain_text  already exists. But that's fine\n"
     ]
    }
   ],
   "source": [
    "def create_plain_text_files(df, dir_with_subs,dir_with_txt):\n",
    "    pwd = os.path.abspath(os.getcwd())\n",
    "    path_from = os.path.join(pwd, dir_with_subs)\n",
    "    path_to = os.path.join(pwd, dir_with_txt)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(path_to)\n",
    "    except:\n",
    "        print(dir_with_txt, ' already exists. But that\\'s fine')\n",
    "        \n",
    "    for i in range(len(labels)):\n",
    "        \n",
    "        \n",
    "        sub_name = labels.iloc[i]['sub_file_name']\n",
    "        \n",
    "        path_to_sub_file = os.path.join(path_from,sub_name)\n",
    "      \n",
    "        sub_obj = pysubs2.load(path_to_sub_file)\n",
    "        \n",
    "        plain_text = create_plain_text_str(sub_obj)\n",
    "        \n",
    "        plain_text_name = sub_name + '.txt'\n",
    "        # print(plain_text_name)\n",
    "        path_to_plain_text_file = os.path.join(path_to,plain_text_name)\n",
    "        \n",
    "        \n",
    "        with open(path_to_plain_text_file, \"w\",encoding=\"utf-8\") as file:\n",
    "            # Writing data to a file\n",
    "            file.write(plain_text)\n",
    "    \n",
    "create_plain_text_files(labels, 'data\\Subtitles_raw','data\\Subtitles_plain_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb62b0-5bc5-4e3e-a81b-ca6c3fcf9c12",
   "metadata": {},
   "source": [
    "## –†–∞—Å–ø—Ä–µ–¥–µ–ª–∏–º txt –ø–æ –ø–æ–¥–ø–∞–ø–∫–∞–º —Å —É—Ä–æ–≤–Ω–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã `sklearn.datasets.load_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c2353bd-7cb2-4bda-b727-06086874c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –≠—Ç–æ –º—É–ª—å—Ç–∫–ª–∞—Å—Å–æ–≤–∞—è —Ä–∞–∑–±–∏–≤–∫–∞\n",
    "# –ï—â–µ —Å—Ç–æ–∏—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –±–∏–Ω–∞—Ä–Ω—ã–µ –∑–∞–¥–∞—á–∏\n",
    "\n",
    "def distribute_into_level_subfolders(labels,dir_with_txt_files,dir_with_level_subfolders):\n",
    "    pwd = os.path.abspath(os.getcwd())\n",
    "    path_from = os.path.join(pwd, dir_with_txt_files)\n",
    "    path_to = os.path.join(pwd, dir_with_level_subfolders)\n",
    "    # print(path_from)\n",
    "    # print(path_to)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(path_to)\n",
    "    except:\n",
    "        print(dir_with_level_subfolders, ' already exists. But that\\'s fine')\n",
    "        \n",
    "    for i in range(len(labels)):\n",
    "        level = labels.iloc[i]['Level']\n",
    "        txt_name = labels.iloc[i]['sub_file_name'] + '.txt'\n",
    "        \n",
    "        # print(level,txt_name)\n",
    "        \n",
    "        path_from_ = os.path.join(path_from,txt_name)\n",
    "        path_to_level = os.path.join(path_to,level)\n",
    "        path_to_file = os.path.join(path_to_level,txt_name)\n",
    "\n",
    "        # print(path_from_)\n",
    "        # print(path_to_)\n",
    "        try:\n",
    "            shutil.copy(path_from_, path_to_file)\n",
    "        except:\n",
    "            os.mkdir(path_to_level)\n",
    "            shutil.copy(path_from_, path_to_file)\n",
    "    \n",
    "distribute_into_level_subfolders(labels,'data\\Subtitles_plain_text','data\\Subtitles_multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87ce893d-625e-456f-9932-3d8036261660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –≠—Ç–æ –º—É–ª—å—Ç–∏ –±–∏–Ω–∞—Ä–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞\n",
    "# –¢—É—Ç —Å—Ç–æ–ª—å–∫–æ –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Å–∫–æ–ª—å–∫–æ —É—Ä–æ–≤–Ω–µ–π\n",
    "# –ü–æ –ø—Ä–∏–Ω—Ü–∏–ø—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ê1 —É—Ä–æ–≤–µ–Ω—å –∏ –Ω–µ –ê1 —É—Ä–æ–≤–µ–Ω—å\n",
    "# –ò —Ç–∞–∫ –ø–æ –∫–∞–∂–¥–æ–º—É —É—Ä–æ–≤–Ω—é\n",
    "\n",
    "def distribute_into_multi_binary_subfolders(labels,dir_with_txt_files,dir_with_multi_binary_subfolders):\n",
    "    pwd = os.path.abspath(os.getcwd())\n",
    "    path_from = os.path.join(pwd, dir_with_txt_files)\n",
    "    path_to = os.path.join(pwd, dir_with_multi_binary_subfolders)\n",
    "    # print(path_from)\n",
    "    # print(path_to)\n",
    "    \n",
    "#     –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –≥–¥–µ –±—É–¥—É—Ç –ª–µ–∂–∞—Ç—å –∫–∞–∂–¥–∞—è –∏–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π\n",
    "    try:\n",
    "        os.mkdir(path_to)\n",
    "    except:\n",
    "        print(dir_with_multi_binary_subfolders, ' already exists. But that\\'s fine')\n",
    "    \n",
    "    for level in labels['Level'].unique():\n",
    "        \n",
    "#         –ë–µ—Ä–µ–º —É—Ä–æ–≤–µ–Ω—å\n",
    "\n",
    "#         –ë–µ—Ä–µ–º —Å—É–±–∏—Ç—Ä—ã —ç—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è\n",
    "        the_level = labels.loc[labels['Level'] == level,'sub_file_name']\n",
    "        the_level = the_level + '.txt'\n",
    "        # display(the_level)\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –ø—É—Ç—å –¥–æ –ø–∞–ø–∫–∏ –≥–¥–µ –±—É–¥—É—Ç —Ä–∞—Å–ø–æ–ª–∞–≥–∞—Ç—å—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "        path_to_level = os.path.join(path_to,level)\n",
    "        # print(path_to_level)\n",
    "        \n",
    "        # –†–∞–∑–º–µ—â–∞—Ç—å—Å—è –æ–Ω–∏ –±—É–¥—É—Ç —Ç–∞–∫:\n",
    "        #     B1/B1\n",
    "        #     B1/not_B1\n",
    "        \n",
    "        # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π –∫–æ–Ω—Ç—Ä–µ—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è \n",
    "        try:\n",
    "            os.mkdir(path_to_level)\n",
    "        except:\n",
    "            print(path_to_level, ' already exists. But that\\'s fine')        \n",
    "        \n",
    "        \n",
    "        for txt in the_level:\n",
    "            # –ö–∞–∂–¥—ã–π —Å—É–±—Ç–∏—Ç—Ä —ç—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–µ—Ä–µ–Ω–æ—Å–∏–º –≤ –ø–∞–ø–∫—É –≤–∏–¥–∞ B1/B1\n",
    "            path_from_file = os.path.join(path_from,txt)\n",
    "            # print(path_from_file)\n",
    "            path_to_level_level = os.path.join(path_to_level,level)\n",
    "            path_to_file = os.path.join(path_to_level_level,txt)\n",
    "            # print(path_to_file)\n",
    "            try:\n",
    "                shutil.copy(path_from_file, path_to_file)\n",
    "            except:\n",
    "                os.mkdir(path_to_level_level)\n",
    "                shutil.copy(path_from_file, path_to_file)\n",
    "            \n",
    "            # try:\n",
    "            #     os.mkdir\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã –Ω–µ —ç—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è\n",
    "        not_level = labels.loc[labels['Level'] != level,'sub_file_name']\n",
    "        not_level = not_level + '.txt'\n",
    "        \n",
    "        for txt in not_level:\n",
    "            # –ö–∞–∂–¥—ã–π —ç—Ç–æ—Ç —Å—É–±—Ç–∏—Ç—Ä –ø–µ—Ä–µ–Ω–æ—Å–∏–º –≤ –ø–∞–ø–∫—É –≤–∏–¥–∞ B1/not_B1\n",
    "            path_from_file = os.path.join(path_from,txt)\n",
    "            # print(path_from_file)\n",
    "            path_to_level_level = os.path.join(path_to_level,'not_' + level)\n",
    "            path_to_file = os.path.join(path_to_level_level,txt)\n",
    "            # print(path_to_file)    \n",
    "            \n",
    "            try:\n",
    "                shutil.copy(path_from_file, path_to_file)\n",
    "            except:\n",
    "                os.mkdir(path_to_level_level)\n",
    "                shutil.copy(path_from_file, path_to_file)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "distribute_into_multi_binary_subfolders(labels,'data\\Subtitles_plain_text','data\\Subtitles_multi_binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb018f1e-18e3-4464-82d7-4d058a604538",
   "metadata": {},
   "source": [
    "## –°–ª–æ–≤–∞—Ä—å —Å —É—Ä–æ–≤–Ω–µ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d06a8b-e53a-4348-b530-f2b92dd3a233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# vocab = pd.read_json('vocab/worddata.json')\n",
    "## vocab = pd.json_normalize('vocab/worddata.json')\n",
    "\n",
    "vocab = pd.read_json('vocab/englishprofile.json')\n",
    "\n",
    "# vocab\n",
    "\n",
    "# vocab[vocab['baseword'] == 'and']\n",
    "\n",
    "# –ß—Ç–æ —Ç–∞–∫–æ–µ `'guideword'` –∏ –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å, —á—Ç–æ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç —Ä–∞–∑–Ω—ã–º —É—Ä–æ–≤–Ω—è–º?\n",
    "\n",
    "# vocab['level'].value_counts()\n",
    "\n",
    "# –ù–∞–º –Ω–µ –Ω—É–∂–Ω—ã —É—Ä–æ–≤–Ω–∏ –≤—ã—à–µ B2, –ø–æ—ç—Ç–æ–º—É –æ–±—Ä–µ–∂–µ–º.\n",
    "\n",
    "# vocab['level'].sort_values().unique().tolist()\n",
    "\n",
    "levels = ['A1', 'A2', 'B1', 'B2']\n",
    "\n",
    "# vocab.loc[vocab['level'].isin(levels)].count()\n",
    "\n",
    "vocab = vocab.loc[vocab['level'].isin(levels)]\n",
    "\n",
    "# vocab\n",
    "\n",
    "# test = vocab.groupby('baseword')['level'].unique()\n",
    "\n",
    "# pd.DataFrame(test).head(50)\n",
    "\n",
    "# with pd.option_context(\"display.max_rows\", 300):\n",
    "#     display(pd.DataFrame(test))\n",
    "\n",
    "# stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebeaef7-c59a-4933-91e1-eff0b2b02738",
   "metadata": {},
   "source": [
    "## –¢–µ—Å—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, IT/IDF, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å–∏–∏ [RAW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1da3f948-d818-467d-bd67-2e9deec133dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SnakeRZR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9658163f-a788-47eb-8e12-fe6f5ad07bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_(path_to_files):\n",
    "    movie_data = load_files(path_to_files)\n",
    "    X, y = movie_data.data, movie_data.target\n",
    "\n",
    "    return X,y\n",
    "\n",
    "X,y = load_data_('data/Subtitles_multi_binary/B1/')\n",
    "# print(len(X))\n",
    "# del X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0c32e8a5-b8c9-4308-98dd-9983d3b6c835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5c23d668-1693-4596-956d-038e80e68c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem(X):\n",
    "    documents = []\n",
    "\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    for sen in range(0, len(X)):\n",
    "    #     # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    #     # Remove single characters from the start\n",
    "    #     document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        document = document.split()\n",
    "\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = ' '.join(document)\n",
    "\n",
    "        documents.append(document)\n",
    "    return documents\n",
    "\n",
    "# len(stem(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "06737edd-0fa3-45a8-8245-9907e19d0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize_(X):\n",
    "    vectorizer = CountVectorizer(\n",
    "                            # max_features=1500, \n",
    "                             # min_df=5, \n",
    "                             # max_df=0.7, \n",
    "                             stop_words=stopwords.words('english'),\n",
    "                            ngram_range = (1,5)\n",
    "                            )\n",
    "    X = vectorizer.fit_transform(X).toarray()\n",
    "    return X\n",
    "# count_vectorize_(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a49f33af-3a51-4ede-a269-1e661f8119c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_transform_(X):\n",
    "    tfidfconverter = TfidfTransformer()\n",
    "    X = tfidfconverter.fit_transform(X).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0846de08-1941-49b8-a236-d3dd12896884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 11395)\n",
      "(18, 11395)\n",
      "[0.5 0.5 0.5 0.5 0.5]\n",
      "[[ 0  6]\n",
      " [ 0 12]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.67      1.00      0.80        12\n",
      "\n",
      "    accuracy                           0.67        18\n",
      "   macro avg       0.33      0.50      0.40        18\n",
      "weighted avg       0.44      0.67      0.53        18\n",
      "\n",
      "\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SnakeRZR\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\SnakeRZR\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\SnakeRZR\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def pipeline_(path_to_files):\n",
    "    X, y = load_data_(path_to_files)\n",
    "    \n",
    "    \n",
    "    # X = stem(X)\n",
    "        \n",
    "    X = count_vectorize_(X)\n",
    "    \n",
    "    \n",
    "    # X = tfidf_transform_(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, \n",
    "                                                    stratify=y\n",
    "                                                   )\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    logreg = LogisticRegression()\n",
    "    \n",
    "    # cv = cross_val_score(logreg,X,y,cv=5,scoring='roc_auc')\n",
    "    # print(cv)\n",
    "    \n",
    "    logreg.fit(X_train,y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print()\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print()\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "pipeline_('data/Subtitles_multi_binary/B1//')    # binary\n",
    "# pipeline_('data/Subtitles_multiclass/')    # multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346e97a-03a7-4fed-bf6f-9844b03cbcda",
   "metadata": {},
   "source": [
    "## –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ç–µ–º–º–µ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2ab8da72-3ae7-4b0f-9756-43af4ed1fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = X[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7dc79476-c33e-45a7-a451-49c66fe9acca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\" Fixed & Synced by bozxphd. Enjoy The Flick  (CLANGING) (DRAWER CLOSES) (INAUDIBLE) (CELL PHONE RINGING) BEN ON PHONE: Michelle, please don\\'t hang up. Just talk to me, okay? I can\\'t believe you just left. Michelle. Come back. Please say something. Michelle, talk to me. Look, we had an argument. Couples fight. That is no reason to just leave everything behind. Running away isn\\'t gonna help it any. Michelle, please... (DIALTONE) NEWSCASTER: More details on that. Elsewhere today, power has still not been restored to many cities on the southern seaboard in the wake of this afternoon\\'s widespread blackout. While there had been some inclement weather in the region, the problem seems linked to what authorities are calling a catastrophic power surge that has crippled traffic in the area. - (LOUD CRASH) - (GRUNTS) - (TIRES SCREECHING) - (SCREAMS) - (GLASS SHATTERING) - (GASPING) (GROANS) (HORN HONKING) (INHALES DEEPLY) (SNIFFS) (SIGHS) (GASPING) (CHAINS RATTLING) (BREATHING HEAVILY) (GRUNTING) \"'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fa8c8f0b-74c9-427e-b778-90134efcd053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\" Fixed & Synced by bozxphd. Enjoy The Flick  (CLANGING) (DRAWER CLOSES) (INAUDIBLE) (CELL PHONE RINGING) BEN ON PHONE: Michelle, please don\\'t hang up. Just talk to me, okay? I can\\'t believe you just left. Michelle. Come back. Please say something. Michelle, talk to me. Look, we had an argument. Couples fight. That is no reason to just leave everything behind. Running away isn\\'t gonna help it any. Michelle, please... (DIALTONE) NEWSCASTER: More details on that. Elsewhere today, power has still not been restored to many cities on the southern seaboard in the wake of this afternoon\\'s widespread blackout. While there had been some inclement weather in the region, the problem seems linked to what authorities are calling a catastrophic power surge that has crippled traffic in the area. - (LOUD CRASH) - (GRUNTS) - (TIRES SCREECHING) - (SCREAMS) - (GLASS SHATTERING) - (GASPING) (GROANS) (HORN HONKING) (INHALES DEEPLY) (SNIFFS) (SIGHS) (GASPING) (CHAINS RATTLING) (BREATHING HEAVILY) (GRUNTING) \"'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = str(test_str)\n",
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7f026efd-49bc-490a-b30e-8856fe2168d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b  Fixed   Synced by bozxphd  Enjoy The Flick   CLANGING   DRAWER CLOSES   INAUDIBLE   CELL PHONE RINGING  BEN ON PHONE  Michelle  please don t hang up  Just talk to me  okay  I can t believe you just left  Michelle  Come back  Please say something  Michelle  talk to me  Look  we had an argument  Couples fight  That is no reason to just leave everything behind  Running away isn t gonna help it any  Michelle  please     DIALTONE  NEWSCASTER  More details on that  Elsewhere today  power has still not been restored to many cities on the southern seaboard in the wake of this afternoon s widespread blackout  While there had been some inclement weather in the region  the problem seems linked to what authorities are calling a catastrophic power surge that has crippled traffic in the area     LOUD CRASH     GRUNTS     TIRES SCREECHING     SCREAMS     GLASS SHATTERING     GASPING   GROANS   HORN HONKING   INHALES DEEPLY   SNIFFS   SIGHS   GASPING   CHAINS RATTLING   BREATHING HEAVILY   GRUNTING   '"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = re.sub(r'\\W', ' ', test_str)\n",
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "72370059-3cb7-46f6-bb1d-6ba9a664e647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b \" Fixed & sync by bozxphd . enjoy the Flick   ( clanging ) ( drawer close ) ( INAUDIBLE ) ( cell phone RINGING ) BEN on phone : Michelle , please do not hang up . just talk to I , okay ? I can not believe you just leave . Michelle . come back . please say something . Michelle , talk to I . look , we have an argument . couple fight . that be no reason to just leave everything behind . run away be not go to help it any . Michelle , please ... ( dialtone ) newscaster : More detail on that . elsewhere today , power have still not be restore to many city on the southern seaboard in the wake of this afternoon \\'s widespread blackout . while there have be some inclement weather in the region , the problem seem link to what authority be call a catastrophic power surge that have cripple traffic in the area . - ( LOUD crash ) - ( grunt ) - ( TIRES SCREECHING ) - ( SCREAMS ) - ( GLASS SHATTERING ) - ( GASPING ) ( GROANS ) ( HORN HONKING ) ( INHALES DEEPLY ) ( SNIFFS ) ( SIGHS ) ( GASPING ) ( chains rattling ) ( BREATHING HEAVILY ) ( grunting ) \"'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(test_str)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])\n",
    "#> 'the strip bat be hang on -PRON- foot for good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "01fb5fdb-48fb-4634-83ce-8f579a116b83",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'lemma_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [185]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma_\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'lemma_'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "71df1887-7bf0-4b05-b2b1-a1db15a403b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a59bad97-75eb-4933-9695-68ca532025b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'Fixed',\n",
       " 'Synced',\n",
       " 'by',\n",
       " 'bozxphd',\n",
       " 'Enjoy',\n",
       " 'The',\n",
       " 'Flick',\n",
       " 'CLANGING',\n",
       " 'DRAWER',\n",
       " 'CLOSES',\n",
       " 'INAUDIBLE',\n",
       " 'CELL',\n",
       " 'PHONE',\n",
       " 'RINGING',\n",
       " 'BEN',\n",
       " 'ON',\n",
       " 'PHONE',\n",
       " 'Michelle',\n",
       " 'please',\n",
       " 'don',\n",
       " 't',\n",
       " 'hang',\n",
       " 'up',\n",
       " 'Just',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'me',\n",
       " 'okay',\n",
       " 'I',\n",
       " 'can',\n",
       " 't',\n",
       " 'believe',\n",
       " 'you',\n",
       " 'just',\n",
       " 'left',\n",
       " 'Michelle',\n",
       " 'Come',\n",
       " 'back',\n",
       " 'Please',\n",
       " 'say',\n",
       " 'something',\n",
       " 'Michelle',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'me',\n",
       " 'Look',\n",
       " 'we',\n",
       " 'had',\n",
       " 'an',\n",
       " 'argument',\n",
       " 'Couples',\n",
       " 'fight',\n",
       " 'That',\n",
       " 'is',\n",
       " 'no',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'just',\n",
       " 'leave',\n",
       " 'everything',\n",
       " 'behind',\n",
       " 'Running',\n",
       " 'away',\n",
       " 'isn',\n",
       " 't',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'help',\n",
       " 'it',\n",
       " 'any',\n",
       " 'Michelle',\n",
       " 'please',\n",
       " 'DIALTONE',\n",
       " 'NEWSCASTER',\n",
       " 'More',\n",
       " 'details',\n",
       " 'on',\n",
       " 'that',\n",
       " 'Elsewhere',\n",
       " 'today',\n",
       " 'power',\n",
       " 'has',\n",
       " 'still',\n",
       " 'not',\n",
       " 'been',\n",
       " 'restored',\n",
       " 'to',\n",
       " 'many',\n",
       " 'cities',\n",
       " 'on',\n",
       " 'the',\n",
       " 'southern',\n",
       " 'seaboard',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wake',\n",
       " 'of',\n",
       " 'this',\n",
       " 'afternoon',\n",
       " 's',\n",
       " 'widespread',\n",
       " 'blackout',\n",
       " 'While',\n",
       " 'there',\n",
       " 'had',\n",
       " 'been',\n",
       " 'some',\n",
       " 'inclement',\n",
       " 'weather',\n",
       " 'in',\n",
       " 'the',\n",
       " 'region',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'seems',\n",
       " 'linked',\n",
       " 'to',\n",
       " 'what',\n",
       " 'authorities',\n",
       " 'are',\n",
       " 'calling',\n",
       " 'a',\n",
       " 'catastrophic',\n",
       " 'power',\n",
       " 'surge',\n",
       " 'that',\n",
       " 'has',\n",
       " 'crippled',\n",
       " 'traffic',\n",
       " 'in',\n",
       " 'the',\n",
       " 'area',\n",
       " 'LOUD',\n",
       " 'CRASH',\n",
       " 'GRUNTS',\n",
       " 'TIRES',\n",
       " 'SCREECHING',\n",
       " 'SCREAMS',\n",
       " 'GLASS',\n",
       " 'SHATTERING',\n",
       " 'GASPING',\n",
       " 'GROANS',\n",
       " 'HORN',\n",
       " 'HONKING',\n",
       " 'INHALES',\n",
       " 'DEEPLY',\n",
       " 'SNIFFS',\n",
       " 'SIGHS',\n",
       " 'GASPING',\n",
       " 'CHAINS',\n",
       " 'RATTLING',\n",
       " 'BREATHING',\n",
       " 'HEAVILY',\n",
       " 'GRUNTING']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = str(test_str)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_list = nltk.word_tokenize(test_str)\n",
    "word_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69f970-e40f-4c7b-899d-a61d3086f46c",
   "metadata": {},
   "source": [
    "## merge countvectorizer + vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852be59-325b-4ed4-b0ec-63d3c561db88",
   "metadata": {},
   "source": [
    "1. –ë–µ—Ä–µ–º —Å–ª–æ–≤–∞—Ä—å\n",
    "2. –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –µ–≥–æ (basewords)\n",
    "3. –ë–µ—Ä–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã (–æ–¥–Ω–æ–≥–æ —Ñ–∏–ª—å–º–∞ –¥–ª—è —Ç–µ—Å—Ç–∞)\n",
    "4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º (—É–±–∏—Ä–∞–µ–º –¥–∏—á—å)\n",
    "5. –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–æ–π –∂–µ —à—Ç—É–∫–æ–π, —á—Ç–æ –∏ —Å–ª–æ–≤–∞—Ä—å\n",
    "6. –ú–µ—Ä–¥–∂–∏–º –ø–æ —Å–ª–æ–≤—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "71827106-5189-42a0-90b4-e9306bedd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_json('vocab/englishprofile.json')\n",
    "\n",
    "levels = ['A1', 'A2', 'B1', 'B2']\n",
    "\n",
    "vocab = vocab.loc[vocab['level'].isin(levels)]\n",
    "\n",
    "vocab = vocab.groupby('baseword')['level'].unique()\n",
    "\n",
    "vocab = pd.DataFrame(vocab)\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "57c85445-17f4-4534-b059-24d67d190a00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [198]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m, disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# sentence = \"The striped bats are hanging on their feet for best\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Parse the sentence using the loaded 'en' model object `nlp`\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Extract the lemma for each token and join\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc])\n",
      "File \u001b[1;32m~\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\spacy\\language.py:1005\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    986\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    989\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    990\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1005\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1007\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\spacy\\language.py:1096\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_doc(doc_like)\n\u001b[1;32m-> 1096\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE866\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(test_str)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])\n",
    "#> 'the strip bat be hang on -PRON- foot for good'\n",
    "\n",
    "def spacy_lemmatize(x,nlp):\n",
    "    str_ = npl(x)\n",
    "    result = \" \".join([token.lemma_ for token in str_])\n",
    "    return result\n",
    "\n",
    "vocab = vocab.reset_index()\n",
    "vocab['lemmas'] = vocab['basewords'].apply(spacy_lemmatize,npl)\n",
    "vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "062d6eb6-62c0-48c9-b67b-a3ce260bf109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseword</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all along</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all in all</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all of a sudden</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>be used to sb/sth/doing sth</td>\n",
       "      <td>[B1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bound to do sth</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6167</th>\n",
       "      <td>zebra</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6168</th>\n",
       "      <td>zero</td>\n",
       "      <td>[A2, B1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6169</th>\n",
       "      <td>zip</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6170</th>\n",
       "      <td>zone</td>\n",
       "      <td>[B1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>zoo</td>\n",
       "      <td>[A1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6172 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           baseword     level\n",
       "0                         all along      [B2]\n",
       "1                        all in all      [B2]\n",
       "2                   all of a sudden      [B2]\n",
       "3      be used to sb/sth/doing sth       [B1]\n",
       "4                   bound to do sth      [B2]\n",
       "...                             ...       ...\n",
       "6167                          zebra      [B2]\n",
       "6168                           zero  [A2, B1]\n",
       "6169                            zip      [B2]\n",
       "6170                           zone      [B1]\n",
       "6171                            zoo      [A1]\n",
       "\n",
       "[6172 rows x 2 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab = vocab.reset_index()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "dd164708-d907-4e12-9178-2876d1e0ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_clean(x):\n",
    "\n",
    "#     # Remove all the special characters\n",
    "    x = re.sub(r'\\W', ' ', x)\n",
    "\n",
    "    # remove all single characters\n",
    "    x = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    x = re.sub(r'\\^[a-zA-Z]\\s+', ' ', x) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    x = re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    x = re.sub(r'^b\\s+', '', x)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    x = x.lower()\n",
    "\n",
    "#     # Lemmatization\n",
    "#     document = document.split()\n",
    "\n",
    "#     document = [stemmer.lemmatize(word) for word in document]\n",
    "#     document = ' '.join(document)\n",
    "\n",
    "#     documents.append(document)\n",
    "    return x\n",
    "\n",
    "# len(stem(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2dec4318-9f49-4bb7-a05a-b6d153dad7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    result = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "    result = \" \".join(result)\n",
    "    return result\n",
    "\n",
    "# df = pd.DataFrame(['this was cheesy', 'she likes these books', 'wow this is great'], columns=['text'])\n",
    "vocab['text_lemmatized'] = vocab['baseword'].apply(str_clean).apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7a261b23-4df1-429d-88d7-beb0f02e96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_string = ' '.join(vocab['baseword'].apply(str_clean).apply(lemmatize_text).unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0ca3970e-b4d3-4114-ba65-bc51b0b1afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vocab_cv = cv.fit_transform([vocab_string])\n",
    "# vocab_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f11960f4-aa1b-4035-bb0f-f81eac543edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '20', 'abandon', ..., 'zip', 'zone', 'zoo'], dtype=object)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "51646b10-a438-4089-996b-1cf01735181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame(vocab_cv.toarray(),columns = cv.get_feature_names_out()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d9edff8f-5b28-4134-84dc-f3964922fccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zebra</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoo</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4623 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "10         2\n",
       "20         1\n",
       "abandon    1\n",
       "abandoned  1\n",
       "ability    1\n",
       "...       ..\n",
       "zebra      1\n",
       "zero       1\n",
       "zip        1\n",
       "zone       1\n",
       "zoo        1\n",
       "\n",
       "[4623 rows x 1 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fd9265cf-333b-4f14-815c-a5a3aae4c65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sth</th>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sb</th>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etc</th>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>up</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>down</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>off</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "sth   614\n",
       "sb    281\n",
       "etc   190\n",
       "in    170\n",
       "or    169\n",
       "up    156\n",
       "the   153\n",
       "to    136\n",
       "of    119\n",
       "be    118\n",
       "out   114\n",
       "on     84\n",
       "do     73\n",
       "down   62\n",
       "get    56\n",
       "take   55\n",
       "off    52\n",
       "for    48\n",
       "with   46\n",
       "your   44"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df.sort_values(0,ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
