{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad640ee-a543-4b1f-8ad4-53dd945e9b0e",
   "metadata": {},
   "source": [
    "Там надо во первых \n",
    "* перевести субтитры в текст, \n",
    "* затем разместить эти текстовые документы по папкам с категориями, чтобы склерн мог их взять как X,y, \n",
    "* затем преобразовывать X (т.е. текст сам), \n",
    "* потом еще попробовать вытащить какую-то мета инфу, типа распределение длинны предложений, распределение длительности появления слов на субтитрах и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ba040-a435-41c4-b837-759a5d58b1dc",
   "metadata": {},
   "source": [
    "1. Создаем датафрейм с названием фильма, уровнем и названием файла с субтитрами\n",
    "    * Удаляем лишние столбцы\n",
    "    * Применяем `.strip()` к названию фильмов (там были переносы строк)\n",
    "    * Заменяем `/` на `_` в названии уровней\n",
    "    * Собираем список файлов с субтитрами с помощью `os.listdir()`\n",
    "    * С помощью `difflib.get_close_matches()` создаем столбец с названиями файлов субтитров\n",
    "2. Создадим из каждого субтитра plain text (.txt) с помощью `pysubs2` в отдельной папке\n",
    "3. Распределим txt по подпапкам с уровнем сложности для корректной работы `sklearn.datasets.load_files`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb6cb1-38b2-40a4-ade2-c59e0fe7a1b6",
   "metadata": {},
   "source": [
    "## Создаем датафрейм с названием фильма, уровнем и названием файла с субтитрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f3197a1-b3bb-44d5-bc00-f8ddee092e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "import difflib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pysubs2\n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9888a108-1764-4e48-9fc9-5e246470bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_preproc(path):\n",
    "    labels = pd.read_csv(path)\n",
    "    # labels.columns\n",
    "    labels = labels.drop(columns=['Kinopoisk ','Subtitles'])\n",
    "    labels = labels.sort_values('Movie').reset_index(drop=True)\n",
    "\n",
    "    labels['Movie'] = labels['Movie'].str.strip()\n",
    "\n",
    "    # with pd.option_context(\"display.max_rows\", 300):\n",
    "    #     display(labels['Movie'])\n",
    "    # labels['Movie'].unique()\n",
    "    # labels['Level'].value_counts()\n",
    "\n",
    "    labels.loc[labels['Level'] == 'A2/A2+','Level'] = 'A2_A2+'\n",
    "    # labels.loc[labels['Level'] == 'B1, B2','Level'] = 'A2_A2+'\n",
    "    labels.loc[labels['Level'] == 'A2/A2+, B1','Level'] = 'A2_A2+, B1'\n",
    "    return labels\n",
    "\n",
    "labels = labels_preproc('data/labels.csv')\n",
    "# labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "533d975b-e163-4377-8687-a0b81690fa29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "      <th>sub_file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Cloverfield Lane</td>\n",
       "      <td>B1</td>\n",
       "      <td>10_Cloverfield_lane(2016).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 things I hate about you</td>\n",
       "      <td>B1</td>\n",
       "      <td>10_things_I_hate_about_you(1999).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A knight’s tale</td>\n",
       "      <td>B2</td>\n",
       "      <td>A_knights_tale(2001).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A star is born</td>\n",
       "      <td>B2</td>\n",
       "      <td>A_star_is_born(2018).srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin</td>\n",
       "      <td>A2_A2+</td>\n",
       "      <td>Aladdin(1992).srt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Movie   Level                         sub_file_name\n",
       "0         10 Cloverfield Lane      B1         10_Cloverfield_lane(2016).srt\n",
       "1  10 things I hate about you      B1  10_things_I_hate_about_you(1999).srt\n",
       "2             A knight’s tale      B2              A_knights_tale(2001).srt\n",
       "3              A star is born      B2              A_star_is_born(2018).srt\n",
       "4                     Aladdin  A2_A2+                     Aladdin(1992).srt"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_subs_file_names(df,path_to_raw_subs):\n",
    "    # Список уже имеющихся субтитров\n",
    "    subs_file_list = os.listdir(path_to_raw_subs)\n",
    "    # subs_file_list = [x.lower() for x in subs_file_list]\n",
    "    # subs_file_list[:5]\n",
    "    # labels.head(1)\n",
    "\n",
    "    # Присваиваем название субтитра по наибольшему совпадению\n",
    "    for movie in df['Movie']:\n",
    "        df.loc[df['Movie'] == movie,'sub_file_name'] = difflib.get_close_matches(movie, subs_file_list,cutoff=0)[0]\n",
    "        \n",
    "    return df\n",
    "        \n",
    "labels = add_subs_file_names(labels,'data/Subtitles_raw/')\n",
    "# labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f69fc-2c25-4e65-ba5a-3e88d4306bee",
   "metadata": {},
   "source": [
    "## Создадим из каждого субтитра plain text (.txt) с помощью `pysubs2` в отдельной папке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab63071-6a54-4163-a7a5-9f35f3695fca",
   "metadata": {},
   "source": [
    "Удалить HTML теги просто\n",
    "https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python#:~:text=If%20you%20need%20to%20strip%20HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c00fc03-a0d5-478d-8832-03833747147a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_plain_text_str(sub_file,delete_html_tags=True):\n",
    "    plain_text = ''\n",
    "    for line in subs:  \n",
    "        plain_text += line.plaintext + ' '\n",
    "        # plain_text += line.text + ' '\n",
    "        \n",
    "    if delete_html_tags:\n",
    "        plain_text = re.sub('<[^<]+?>', ' ', plain_text)\n",
    "        \n",
    "    return plain_text\n",
    "\n",
    "\n",
    "# subs = pysubs2.load('data/Subtitles_raw/10_Cloverfield_lane(2016).srt',\n",
    "#                     # keep_html_tags=False,\n",
    "#                     keep_unknown_html_tags=True\n",
    "#                    )\n",
    "\n",
    "# test = create_plain_text_str(subs)\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e908ec3a-572a-4aa9-b2f5-bb05a4f57085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plain_text_files(df, dir_with_subs,dir_with_txt):\n",
    "    pwd = os.path.abspath(os.getcwd())\n",
    "    path_from = os.path.join(pwd, dir_with_subs)\n",
    "    path_to = os.path.join(pwd, dir_with_txt)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(path_to)\n",
    "    except:\n",
    "        print(dir_with_txt, ' already exists. But that\\'s fine')\n",
    "        \n",
    "    for i in range(len(labels)):\n",
    "        \n",
    "        sub_name = labels.iloc[i]['sub_file_name']\n",
    "        \n",
    "        path_to_sub_file = os.path.join(path_from,sub_name)\n",
    "      \n",
    "        sub_obj = pysubs2.load(path_to_sub_file)\n",
    "        \n",
    "        plain_text = create_plain_text_str(sub_obj)\n",
    "        \n",
    "        plain_text_name = sub_name + '.txt'\n",
    "        \n",
    "        path_to_plain_text_file = os.path.join(path_to,plain_text_name)\n",
    "        \n",
    "        \n",
    "        with open(path_to_plain_text_file, \"w\") as file:\n",
    "            # Writing data to a file\n",
    "            file.write(plain_text)\n",
    "    \n",
    "# create_plain_text_files(labels, 'data\\Subtitles_raw','data\\Subtitles_plain_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb62b0-5bc5-4e3e-a81b-ca6c3fcf9c12",
   "metadata": {},
   "source": [
    "## Распределим txt по подпапкам с уровнем сложности для корректной работы `sklearn.datasets.load_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2353bd-7cb2-4bda-b727-06086874c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Это мультклассовая разбивка\n",
    "# Еще стоит разделить на бинарные задачи\n",
    "\n",
    "def distribute_into_level_subfolders(labels,dir_with_txt_files,dir_with_level_subfolders):\n",
    "    pwd = os.path.abspath(os.getcwd())\n",
    "    path_from = os.path.join(pwd, dir_with_txt_files)\n",
    "    path_to = os.path.join(pwd, dir_with_level_subfolders)\n",
    "    # print(path_from)\n",
    "    # print(path_to)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(path_to)\n",
    "    except:\n",
    "        print(dir_with_level_subfolders, ' already exists. But that\\'s fine')\n",
    "        \n",
    "    for i in range(len(labels)):\n",
    "        level = labels.iloc[i]['Level']\n",
    "        txt_name = labels.iloc[i]['sub_file_name'] + '.txt'\n",
    "        \n",
    "        # print(level,txt_name)\n",
    "        \n",
    "        path_from_ = os.path.join(path_from,txt_name)\n",
    "        path_to_level = os.path.join(path_to,level)\n",
    "        path_to_file = os.path.join(path_to_level,txt_name)\n",
    "\n",
    "        # print(path_from_)\n",
    "        # print(path_to_)\n",
    "        try:\n",
    "            shutil.copy(path_from_, path_to_file)\n",
    "        except:\n",
    "            os.mkdir(path_to_level)\n",
    "            shutil.copy(path_from_, path_to_file)\n",
    "    \n",
    "# distribute_into_level_subfolders(labels,'data\\Subtitles_plain_text','data\\Subtitles_multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87ce893d-625e-456f-9932-3d8036261660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Это мульти бинарная разбивка\n",
    "# Тут столько задач классификации, сколько уровней\n",
    "# По принципу определения А1 уровень и не А1 уровень\n",
    "# И так по каждому уровню\n",
    "\n",
    "def distribute_into_multi_binary_subfolders(labels,dir_with_txt_files,dir_with_multi_binary_subfolders):\n",
    "    pwd = os.path.abspath(os.getcwd())\n",
    "    path_from = os.path.join(pwd, dir_with_txt_files)\n",
    "    path_to = os.path.join(pwd, dir_with_multi_binary_subfolders)\n",
    "    # print(path_from)\n",
    "    # print(path_to)\n",
    "    \n",
    "#     Создаем папку где будут лежать каждая из классификаций\n",
    "    try:\n",
    "        os.mkdir(path_to)\n",
    "    except:\n",
    "        print(dir_with_multi_binary_subfolders, ' already exists. But that\\'s fine')\n",
    "    \n",
    "    for level in labels['Level'].unique():\n",
    "        \n",
    "#         Берем уровень\n",
    "\n",
    "#         Берем субитры этого уровня\n",
    "        the_level = labels.loc[labels['Level'] == level,'sub_file_name']\n",
    "        the_level = the_level + '.txt'\n",
    "        # display(the_level)\n",
    "        \n",
    "        # Берем путь до папки где будут располагаться категории\n",
    "        path_to_level = os.path.join(path_to,level)\n",
    "        # print(path_to_level)\n",
    "        \n",
    "        # Размещаться они будут так:\n",
    "        #     B1/B1\n",
    "        #     B1/not_B1\n",
    "        \n",
    "        # Пробуем создать папку с категорией контретного уровня \n",
    "        try:\n",
    "            os.mkdir(path_to_level)\n",
    "        except:\n",
    "            print(path_to_level, ' already exists. But that\\'s fine')        \n",
    "        \n",
    "        \n",
    "        for txt in the_level:\n",
    "            # Каждый субтитр этого уровня переносим в папку вида B1/B1\n",
    "            path_from_file = os.path.join(path_from,txt)\n",
    "            # print(path_from_file)\n",
    "            path_to_level_level = os.path.join(path_to_level,level)\n",
    "            path_to_file = os.path.join(path_to_level_level,txt)\n",
    "            # print(path_to_file)\n",
    "            try:\n",
    "                shutil.copy(path_from_file, path_to_file)\n",
    "            except:\n",
    "                os.mkdir(path_to_level_level)\n",
    "                shutil.copy(path_from_file, path_to_file)\n",
    "            \n",
    "            # try:\n",
    "            #     os.mkdir\n",
    "        \n",
    "        # Берем субтитры не этого уровня\n",
    "        not_level = labels.loc[labels['Level'] != level,'sub_file_name']\n",
    "        not_level = not_level + '.txt'\n",
    "        \n",
    "        for txt in not_level:\n",
    "            # Каждый этот субтитр переносим в папку вида B1/not_B1\n",
    "            path_from_file = os.path.join(path_from,txt)\n",
    "            # print(path_from_file)\n",
    "            path_to_level_level = os.path.join(path_to_level,'not_' + level)\n",
    "            path_to_file = os.path.join(path_to_level_level,txt)\n",
    "            # print(path_to_file)    \n",
    "            \n",
    "            try:\n",
    "                shutil.copy(path_from_file, path_to_file)\n",
    "            except:\n",
    "                os.mkdir(path_to_level_level)\n",
    "                shutil.copy(path_from_file, path_to_file)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "# distribute_into_multi_binary_subfolders(labels,'data\\Subtitles_plain_text','data\\Subtitles_multi_binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb018f1e-18e3-4464-82d7-4d058a604538",
   "metadata": {},
   "source": [
    "## Словарь с уровнем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d06a8b-e53a-4348-b530-f2b92dd3a233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# vocab = pd.read_json('vocab/worddata.json')\n",
    "## vocab = pd.json_normalize('vocab/worddata.json')\n",
    "\n",
    "vocab = pd.read_json('vocab/englishprofile.json')\n",
    "\n",
    "# vocab\n",
    "\n",
    "# vocab[vocab['baseword'] == 'and']\n",
    "\n",
    "# Что такое `'guideword'` и как обрабатывать, что одно слово принадлежит разным уровням?\n",
    "\n",
    "# vocab['level'].value_counts()\n",
    "\n",
    "# Нам не нужны уровни выше B2, поэтому обрежем.\n",
    "\n",
    "# vocab['level'].sort_values().unique().tolist()\n",
    "\n",
    "levels = ['A1', 'A2', 'B1', 'B2']\n",
    "\n",
    "# vocab.loc[vocab['level'].isin(levels)].count()\n",
    "\n",
    "vocab = vocab.loc[vocab['level'].isin(levels)]\n",
    "\n",
    "# vocab\n",
    "\n",
    "# test = vocab.groupby('baseword')['level'].unique()\n",
    "\n",
    "# pd.DataFrame(test).head(50)\n",
    "\n",
    "# with pd.option_context(\"display.max_rows\", 300):\n",
    "#     display(pd.DataFrame(test))\n",
    "\n",
    "# stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebeaef7-c59a-4933-91e1-eff0b2b02738",
   "metadata": {},
   "source": [
    "## Тест векторизации текста, IT/IDF, логистической регресии [RAW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1da3f948-d818-467d-bd67-2e9deec133dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SnakeRZR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9658163f-a788-47eb-8e12-fe6f5ad07bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_(path_to_files):\n",
    "    movie_data = load_files(path_to_files)\n",
    "    X, y = movie_data.data, movie_data.target\n",
    "\n",
    "    return X,y\n",
    "\n",
    "X,y = load_data_('data/Subtitles_multi_binary/B1/')\n",
    "# print(len(X))\n",
    "# del X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0c32e8a5-b8c9-4308-98dd-9983d3b6c835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5c23d668-1693-4596-956d-038e80e68c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem(X):\n",
    "    documents = []\n",
    "\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    for sen in range(0, len(X)):\n",
    "    #     # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    #     # Remove single characters from the start\n",
    "    #     document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        document = document.split()\n",
    "\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = ' '.join(document)\n",
    "\n",
    "        documents.append(document)\n",
    "    return documents\n",
    "\n",
    "# len(stem(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "06737edd-0fa3-45a8-8245-9907e19d0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize_(X):\n",
    "    vectorizer = CountVectorizer(\n",
    "                            # max_features=1500, \n",
    "                             # min_df=5, \n",
    "                             # max_df=0.7, \n",
    "                             stop_words=stopwords.words('english'),\n",
    "                            ngram_range = (1,5)\n",
    "                            )\n",
    "    X = vectorizer.fit_transform(X).toarray()\n",
    "    return X\n",
    "# count_vectorize_(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a49f33af-3a51-4ede-a269-1e661f8119c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_transform_(X):\n",
    "    tfidfconverter = TfidfTransformer()\n",
    "    X = tfidfconverter.fit_transform(X).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0846de08-1941-49b8-a236-d3dd12896884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 11395)\n",
      "(18, 11395)\n",
      "[0.5 0.5 0.5 0.5 0.5]\n",
      "[[ 0  6]\n",
      " [ 0 12]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.67      1.00      0.80        12\n",
      "\n",
      "    accuracy                           0.67        18\n",
      "   macro avg       0.33      0.50      0.40        18\n",
      "weighted avg       0.44      0.67      0.53        18\n",
      "\n",
      "\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SnakeRZR\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\SnakeRZR\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\SnakeRZR\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def pipeline_(path_to_files):\n",
    "    X, y = load_data_(path_to_files)\n",
    "    \n",
    "    \n",
    "    # X = stem(X)\n",
    "        \n",
    "    X = count_vectorize_(X)\n",
    "    \n",
    "    \n",
    "    # X = tfidf_transform_(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, \n",
    "                                                    stratify=y\n",
    "                                                   )\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    logreg = LogisticRegression()\n",
    "    \n",
    "    # cv = cross_val_score(logreg,X,y,cv=5,scoring='roc_auc')\n",
    "    # print(cv)\n",
    "    \n",
    "    logreg.fit(X_train,y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print()\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print()\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "pipeline_('data/Subtitles_multi_binary/B1//')    # binary\n",
    "# pipeline_('data/Subtitles_multiclass/')    # multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346e97a-03a7-4fed-bf6f-9844b03cbcda",
   "metadata": {},
   "source": [
    "## Исследование стеммера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2ab8da72-3ae7-4b0f-9756-43af4ed1fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = X[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7dc79476-c33e-45a7-a451-49c66fe9acca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\" Fixed & Synced by bozxphd. Enjoy The Flick  (CLANGING) (DRAWER CLOSES) (INAUDIBLE) (CELL PHONE RINGING) BEN ON PHONE: Michelle, please don\\'t hang up. Just talk to me, okay? I can\\'t believe you just left. Michelle. Come back. Please say something. Michelle, talk to me. Look, we had an argument. Couples fight. That is no reason to just leave everything behind. Running away isn\\'t gonna help it any. Michelle, please... (DIALTONE) NEWSCASTER: More details on that. Elsewhere today, power has still not been restored to many cities on the southern seaboard in the wake of this afternoon\\'s widespread blackout. While there had been some inclement weather in the region, the problem seems linked to what authorities are calling a catastrophic power surge that has crippled traffic in the area. - (LOUD CRASH) - (GRUNTS) - (TIRES SCREECHING) - (SCREAMS) - (GLASS SHATTERING) - (GASPING) (GROANS) (HORN HONKING) (INHALES DEEPLY) (SNIFFS) (SIGHS) (GASPING) (CHAINS RATTLING) (BREATHING HEAVILY) (GRUNTING) \"'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fa8c8f0b-74c9-427e-b778-90134efcd053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\" Fixed & Synced by bozxphd. Enjoy The Flick  (CLANGING) (DRAWER CLOSES) (INAUDIBLE) (CELL PHONE RINGING) BEN ON PHONE: Michelle, please don\\'t hang up. Just talk to me, okay? I can\\'t believe you just left. Michelle. Come back. Please say something. Michelle, talk to me. Look, we had an argument. Couples fight. That is no reason to just leave everything behind. Running away isn\\'t gonna help it any. Michelle, please... (DIALTONE) NEWSCASTER: More details on that. Elsewhere today, power has still not been restored to many cities on the southern seaboard in the wake of this afternoon\\'s widespread blackout. While there had been some inclement weather in the region, the problem seems linked to what authorities are calling a catastrophic power surge that has crippled traffic in the area. - (LOUD CRASH) - (GRUNTS) - (TIRES SCREECHING) - (SCREAMS) - (GLASS SHATTERING) - (GASPING) (GROANS) (HORN HONKING) (INHALES DEEPLY) (SNIFFS) (SIGHS) (GASPING) (CHAINS RATTLING) (BREATHING HEAVILY) (GRUNTING) \"'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = str(test_str)\n",
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7f026efd-49bc-490a-b30e-8856fe2168d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b  Fixed   Synced by bozxphd  Enjoy The Flick   CLANGING   DRAWER CLOSES   INAUDIBLE   CELL PHONE RINGING  BEN ON PHONE  Michelle  please don t hang up  Just talk to me  okay  I can t believe you just left  Michelle  Come back  Please say something  Michelle  talk to me  Look  we had an argument  Couples fight  That is no reason to just leave everything behind  Running away isn t gonna help it any  Michelle  please     DIALTONE  NEWSCASTER  More details on that  Elsewhere today  power has still not been restored to many cities on the southern seaboard in the wake of this afternoon s widespread blackout  While there had been some inclement weather in the region  the problem seems linked to what authorities are calling a catastrophic power surge that has crippled traffic in the area     LOUD CRASH     GRUNTS     TIRES SCREECHING     SCREAMS     GLASS SHATTERING     GASPING   GROANS   HORN HONKING   INHALES DEEPLY   SNIFFS   SIGHS   GASPING   CHAINS RATTLING   BREATHING HEAVILY   GRUNTING   '"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = re.sub(r'\\W', ' ', test_str)\n",
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "72370059-3cb7-46f6-bb1d-6ba9a664e647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b \" Fixed & sync by bozxphd . enjoy the Flick   ( clanging ) ( drawer close ) ( INAUDIBLE ) ( cell phone RINGING ) BEN on phone : Michelle , please do not hang up . just talk to I , okay ? I can not believe you just leave . Michelle . come back . please say something . Michelle , talk to I . look , we have an argument . couple fight . that be no reason to just leave everything behind . run away be not go to help it any . Michelle , please ... ( dialtone ) newscaster : More detail on that . elsewhere today , power have still not be restore to many city on the southern seaboard in the wake of this afternoon \\'s widespread blackout . while there have be some inclement weather in the region , the problem seem link to what authority be call a catastrophic power surge that have cripple traffic in the area . - ( LOUD crash ) - ( grunt ) - ( TIRES SCREECHING ) - ( SCREAMS ) - ( GLASS SHATTERING ) - ( GASPING ) ( GROANS ) ( HORN HONKING ) ( INHALES DEEPLY ) ( SNIFFS ) ( SIGHS ) ( GASPING ) ( chains rattling ) ( BREATHING HEAVILY ) ( grunting ) \"'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(test_str)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])\n",
    "#> 'the strip bat be hang on -PRON- foot for good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "01fb5fdb-48fb-4634-83ce-8f579a116b83",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'lemma_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [185]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma_\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'lemma_'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "71df1887-7bf0-4b05-b2b1-a1db15a403b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a59bad97-75eb-4933-9695-68ca532025b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'Fixed',\n",
       " 'Synced',\n",
       " 'by',\n",
       " 'bozxphd',\n",
       " 'Enjoy',\n",
       " 'The',\n",
       " 'Flick',\n",
       " 'CLANGING',\n",
       " 'DRAWER',\n",
       " 'CLOSES',\n",
       " 'INAUDIBLE',\n",
       " 'CELL',\n",
       " 'PHONE',\n",
       " 'RINGING',\n",
       " 'BEN',\n",
       " 'ON',\n",
       " 'PHONE',\n",
       " 'Michelle',\n",
       " 'please',\n",
       " 'don',\n",
       " 't',\n",
       " 'hang',\n",
       " 'up',\n",
       " 'Just',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'me',\n",
       " 'okay',\n",
       " 'I',\n",
       " 'can',\n",
       " 't',\n",
       " 'believe',\n",
       " 'you',\n",
       " 'just',\n",
       " 'left',\n",
       " 'Michelle',\n",
       " 'Come',\n",
       " 'back',\n",
       " 'Please',\n",
       " 'say',\n",
       " 'something',\n",
       " 'Michelle',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'me',\n",
       " 'Look',\n",
       " 'we',\n",
       " 'had',\n",
       " 'an',\n",
       " 'argument',\n",
       " 'Couples',\n",
       " 'fight',\n",
       " 'That',\n",
       " 'is',\n",
       " 'no',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'just',\n",
       " 'leave',\n",
       " 'everything',\n",
       " 'behind',\n",
       " 'Running',\n",
       " 'away',\n",
       " 'isn',\n",
       " 't',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'help',\n",
       " 'it',\n",
       " 'any',\n",
       " 'Michelle',\n",
       " 'please',\n",
       " 'DIALTONE',\n",
       " 'NEWSCASTER',\n",
       " 'More',\n",
       " 'details',\n",
       " 'on',\n",
       " 'that',\n",
       " 'Elsewhere',\n",
       " 'today',\n",
       " 'power',\n",
       " 'has',\n",
       " 'still',\n",
       " 'not',\n",
       " 'been',\n",
       " 'restored',\n",
       " 'to',\n",
       " 'many',\n",
       " 'cities',\n",
       " 'on',\n",
       " 'the',\n",
       " 'southern',\n",
       " 'seaboard',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wake',\n",
       " 'of',\n",
       " 'this',\n",
       " 'afternoon',\n",
       " 's',\n",
       " 'widespread',\n",
       " 'blackout',\n",
       " 'While',\n",
       " 'there',\n",
       " 'had',\n",
       " 'been',\n",
       " 'some',\n",
       " 'inclement',\n",
       " 'weather',\n",
       " 'in',\n",
       " 'the',\n",
       " 'region',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'seems',\n",
       " 'linked',\n",
       " 'to',\n",
       " 'what',\n",
       " 'authorities',\n",
       " 'are',\n",
       " 'calling',\n",
       " 'a',\n",
       " 'catastrophic',\n",
       " 'power',\n",
       " 'surge',\n",
       " 'that',\n",
       " 'has',\n",
       " 'crippled',\n",
       " 'traffic',\n",
       " 'in',\n",
       " 'the',\n",
       " 'area',\n",
       " 'LOUD',\n",
       " 'CRASH',\n",
       " 'GRUNTS',\n",
       " 'TIRES',\n",
       " 'SCREECHING',\n",
       " 'SCREAMS',\n",
       " 'GLASS',\n",
       " 'SHATTERING',\n",
       " 'GASPING',\n",
       " 'GROANS',\n",
       " 'HORN',\n",
       " 'HONKING',\n",
       " 'INHALES',\n",
       " 'DEEPLY',\n",
       " 'SNIFFS',\n",
       " 'SIGHS',\n",
       " 'GASPING',\n",
       " 'CHAINS',\n",
       " 'RATTLING',\n",
       " 'BREATHING',\n",
       " 'HEAVILY',\n",
       " 'GRUNTING']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = str(test_str)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_list = nltk.word_tokenize(test_str)\n",
    "word_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69f970-e40f-4c7b-899d-a61d3086f46c",
   "metadata": {},
   "source": [
    "## merge countvectorizer + vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852be59-325b-4ed4-b0ec-63d3c561db88",
   "metadata": {},
   "source": [
    "1. Берем словарь\n",
    "2. Лемматизируем его (basewords)\n",
    "3. Берем субтитры (одного фильма для теста)\n",
    "4. Обрабатываем (убираем дичь)\n",
    "5. Лемматизируем той же штукой, что и словарь\n",
    "6. Мерджим по слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "71827106-5189-42a0-90b4-e9306bedd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_json('vocab/englishprofile.json')\n",
    "\n",
    "levels = ['A1', 'A2', 'B1', 'B2']\n",
    "\n",
    "vocab = vocab.loc[vocab['level'].isin(levels)]\n",
    "\n",
    "vocab = vocab.groupby('baseword')['level'].unique()\n",
    "\n",
    "vocab = pd.DataFrame(vocab)\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "57c85445-17f4-4534-b059-24d67d190a00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [198]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m, disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# sentence = \"The striped bats are hanging on their feet for best\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Parse the sentence using the loaded 'en' model object `nlp`\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Extract the lemma for each token and join\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc])\n",
      "File \u001b[1;32m~\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\spacy\\language.py:1005\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    986\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    989\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    990\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1005\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1007\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\Desktop\\projects\\film_level_estimator\\env\\lib\\site-packages\\spacy\\language.py:1096\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_doc(doc_like)\n\u001b[1;32m-> 1096\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE866\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(test_str)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])\n",
    "#> 'the strip bat be hang on -PRON- foot for good'\n",
    "\n",
    "def spacy_lemmatize(x,nlp):\n",
    "    str_ = npl(x)\n",
    "    result = \" \".join([token.lemma_ for token in str_])\n",
    "    return result\n",
    "\n",
    "vocab = vocab.reset_index()\n",
    "vocab['lemmas'] = vocab['basewords'].apply(spacy_lemmatize,npl)\n",
    "vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "062d6eb6-62c0-48c9-b67b-a3ce260bf109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseword</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all along</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all in all</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all of a sudden</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>be used to sb/sth/doing sth</td>\n",
       "      <td>[B1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bound to do sth</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6167</th>\n",
       "      <td>zebra</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6168</th>\n",
       "      <td>zero</td>\n",
       "      <td>[A2, B1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6169</th>\n",
       "      <td>zip</td>\n",
       "      <td>[B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6170</th>\n",
       "      <td>zone</td>\n",
       "      <td>[B1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>zoo</td>\n",
       "      <td>[A1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6172 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           baseword     level\n",
       "0                         all along      [B2]\n",
       "1                        all in all      [B2]\n",
       "2                   all of a sudden      [B2]\n",
       "3      be used to sb/sth/doing sth       [B1]\n",
       "4                   bound to do sth      [B2]\n",
       "...                             ...       ...\n",
       "6167                          zebra      [B2]\n",
       "6168                           zero  [A2, B1]\n",
       "6169                            zip      [B2]\n",
       "6170                           zone      [B1]\n",
       "6171                            zoo      [A1]\n",
       "\n",
       "[6172 rows x 2 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab = vocab.reset_index()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "dd164708-d907-4e12-9178-2876d1e0ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_clean(x):\n",
    "\n",
    "#     # Remove all the special characters\n",
    "    x = re.sub(r'\\W', ' ', x)\n",
    "\n",
    "    # remove all single characters\n",
    "    x = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    x = re.sub(r'\\^[a-zA-Z]\\s+', ' ', x) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    x = re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    x = re.sub(r'^b\\s+', '', x)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    x = x.lower()\n",
    "\n",
    "#     # Lemmatization\n",
    "#     document = document.split()\n",
    "\n",
    "#     document = [stemmer.lemmatize(word) for word in document]\n",
    "#     document = ' '.join(document)\n",
    "\n",
    "#     documents.append(document)\n",
    "    return x\n",
    "\n",
    "# len(stem(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2dec4318-9f49-4bb7-a05a-b6d153dad7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    result = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "    result = \" \".join(result)\n",
    "    return result\n",
    "\n",
    "# df = pd.DataFrame(['this was cheesy', 'she likes these books', 'wow this is great'], columns=['text'])\n",
    "vocab['text_lemmatized'] = vocab['baseword'].apply(str_clean).apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7a261b23-4df1-429d-88d7-beb0f02e96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_string = ' '.join(vocab['baseword'].apply(str_clean).apply(lemmatize_text).unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0ca3970e-b4d3-4114-ba65-bc51b0b1afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vocab_cv = cv.fit_transform([vocab_string])\n",
    "# vocab_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f11960f4-aa1b-4035-bb0f-f81eac543edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '20', 'abandon', ..., 'zip', 'zone', 'zoo'], dtype=object)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "51646b10-a438-4089-996b-1cf01735181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame(vocab_cv.toarray(),columns = cv.get_feature_names_out()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d9edff8f-5b28-4134-84dc-f3964922fccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zebra</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoo</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4623 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "10         2\n",
       "20         1\n",
       "abandon    1\n",
       "abandoned  1\n",
       "ability    1\n",
       "...       ..\n",
       "zebra      1\n",
       "zero       1\n",
       "zip        1\n",
       "zone       1\n",
       "zoo        1\n",
       "\n",
       "[4623 rows x 1 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fd9265cf-333b-4f14-815c-a5a3aae4c65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sth</th>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sb</th>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etc</th>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>up</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>down</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>off</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "sth   614\n",
       "sb    281\n",
       "etc   190\n",
       "in    170\n",
       "or    169\n",
       "up    156\n",
       "the   153\n",
       "to    136\n",
       "of    119\n",
       "be    118\n",
       "out   114\n",
       "on     84\n",
       "do     73\n",
       "down   62\n",
       "get    56\n",
       "take   55\n",
       "off    52\n",
       "for    48\n",
       "with   46\n",
       "your   44"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df.sort_values(0,ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
